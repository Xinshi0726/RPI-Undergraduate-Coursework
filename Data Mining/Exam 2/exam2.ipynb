{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4e0033-ee59-40df-84ad-5a159deb421a",
   "metadata": {},
   "source": [
    "## Exam II: CSCI4390-6390 (100 points)\n",
    "\n",
    "This is a take-home exam. It is due 11:59:59pm on Thursday, Nov 3 via submitty. You are expected to abide by the honor code, i.e., all work must be your own, and you are not allowed to discuss any aspect of exam with anyone except the TA or the professor. You are also not allowed to use the internet except to lookup numpy/python documentation, the book site, class notes/videos, or to ask clarification questions on campuswire. **You cannot use any other library other than numpy (e.g., no pytorch, sklearn, etc); you can use pandas only for data processing**. You must sign the declaration below that you will abide by the honor code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78163f5-20e5-427d-9d62-977d2ee7a1e6",
   "metadata": {},
   "source": [
    "#### **Honor Code Declaration**: Please sign with your name to acknowledge that you agree to abide by the honor code, and demonstrate the highest level of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536081-83a0-4b65-b3e7-557e384915db",
   "metadata": {},
   "source": [
    "Xinshi Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c78be-4a74-44c1-818f-8f5bb77fc69c",
   "metadata": {},
   "source": [
    "#### Download Dataset: Download the [Seeds Dataset](https://archive.ics.uci.edu/ml/datasets/seeds). It has 9 attributes, and 210 points. The last attribute is the class variable. Store the dataset in your current directory and use \"./seeds_dataset.txt\" as its name. **DO NOT** submit the dataset as part of your answer on submitty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "88ce8a96-6615-4543-a445-a53b8ca6922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "dataset = open('seeds_dataset.txt', 'r')\n",
    "dataset_list = []\n",
    "for line in dataset:\n",
    "    data = line.strip().split('\\t')\n",
    "    while '' in data:\n",
    "        data.remove('')\n",
    "    assert(len(data) == 8)\n",
    "    dataset_list.append([float(data[i]) for i in range(8)])\n",
    "dataset = np.array(pd.DataFrame(data = np.array(dataset_list), columns=range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "02edae80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.26  , 14.84  ,  0.871 , ...,  2.221 ,  5.22  ,  1.    ],\n",
       "       [14.88  , 14.57  ,  0.8811, ...,  1.018 ,  4.956 ,  1.    ],\n",
       "       [14.29  , 14.09  ,  0.905 , ...,  2.699 ,  4.825 ,  1.    ],\n",
       "       ...,\n",
       "       [13.2   , 13.66  ,  0.8883, ...,  8.315 ,  5.056 ,  3.    ],\n",
       "       [11.84  , 13.21  ,  0.8521, ...,  3.598 ,  5.044 ,  3.    ],\n",
       "       [12.3   , 13.34  ,  0.8684, ...,  5.637 ,  5.063 ,  3.    ]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4632d-993f-4479-bbd5-2996fab7aa40",
   "metadata": {},
   "source": [
    "#### Q1. Linear Regression: (Total 25 points)  For this question, we will use the first attribute as the target, and you will ignore the last attribute, which is the class. We will use linear regression to predict the target via the geometric approach. Answer the following:\n",
    "\n",
    "##### a) (10 points) First find and print the orthogonal basis vectors (without unit normalization) for the independent attributes; print the first three and last three values of each basis vector. You cannot call numpy QR or simialr function for this.\n",
    "\n",
    "##### b) (5 points) Next find and print the predicted vector, and show the squared error. You may not use numpy QR or similar function for this problem. \n",
    "\n",
    "##### c) (5 points) Find and print the regression coefficient or weight vector, using any method of choice. You can use the numpy inv function.\n",
    "\n",
    "##### d) (5 points) Plot the scatter plot and regression line for the target versus the first attribute. Note: do not center the data, and do not run a separate regression for this. You must plot the effect of 1st attribute based on the full multivariate regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f190b7d",
   "metadata": {},
   "source": [
    "a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d78ef0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_by_one(data):\n",
    "    ones = np.ones(data.shape[0]).reshape(-1,1)\n",
    "    augmented_data = np.hstack((ones,data))\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "536deb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_x = augment_by_one(np.copy(dataset[:,1:7]))\n",
    "q1_train_y = np.copy(dataset[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "11b5efdc-55de-4555-9198-e39c4b8bfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(X_i,U_j):\n",
    "    return np.dot(X_i.T,U_j)/(np.dot(U_j.T,U_j))\n",
    "def norm(X):\n",
    "    return np.sqrt(np.dot(X.T,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "cf987b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros(shape = q1_train_x.shape)\n",
    "for i in range(q1_train_x.shape[1]):\n",
    "    X_i = np.array(q1_train_x[:,i],copy = True,dtype=np.float64)\n",
    "    X_i_proj = np.array(q1_train_x[:,i],copy = True,dtype=np.float64)\n",
    "    j = 0\n",
    "    while j < i:\n",
    "        U_j = np.array(Q[:,j],copy = True)\n",
    "        X_i -= proj(X_i_proj,U_j)*U_j\n",
    "        j += 1\n",
    "    Q[:,i] = X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "07407f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.eye(q1_train_x.shape[1],dtype = np.float64)\n",
    "for i in range(q1_train_x.shape[1]):\n",
    "    for j in range(i+1,q1_train_x.shape[1]):\n",
    "        R[i,j] = proj(q1_train_x[:,j],Q[:,i]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5014e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three vectors are:\n",
      "[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [ 2.80714286e-01  1.07142857e-02 -4.69285714e-01]\n",
      " [-2.68666067e-03  9.99882975e-03  3.84952572e-02]\n",
      " [ 3.15891940e-02 -3.98544924e-02 -3.55914391e-02]\n",
      " [-2.16672272e-03  6.39689071e-03 -2.32061234e-02]\n",
      " [-1.42209631e+00 -2.56912585e+00 -1.96882473e-01]\n",
      " [-2.62296882e-01 -2.41113780e-01 -1.18489995e-01]]\n",
      "The last three vectors are:\n",
      "[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [-8.99285714e-01 -1.34928571e+00 -1.21928571e+00]\n",
      " [ 2.59128900e-02 -5.97795927e-03  9.07717498e-03]\n",
      " [ 3.18120675e-03 -3.12412250e-02  5.14086335e-02]\n",
      " [ 6.91857121e-02 -2.46484383e-02  4.88237671e-03]\n",
      " [ 4.27131323e+00 -3.53448753e-01  1.79274780e+00]\n",
      " [-5.72193143e-02  8.47947022e-02  9.73647506e-03]]\n"
     ]
    }
   ],
   "source": [
    "print('The first three vectors are:')\n",
    "print(Q[:3,:].T)\n",
    "print('The last three vectors are:')\n",
    "print(Q[-3:,:].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d9c37",
   "metadata": {},
   "source": [
    "b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "48f0d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Q.shape[1]):\n",
    "    if (i == 0):\n",
    "        result = proj(q1_train_y,Q[:,i])*Q[:,i]\n",
    "    else:\n",
    "        result += proj(q1_train_y,Q[:,i])*Q[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "00802ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSE(pred,y):\n",
    "    return np.sum(np.square(pred-y))\n",
    "def TSS(pred,y):\n",
    "    return np.sum(np.square(pred-y.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "08b93f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE is 2.522217722717802\n"
     ]
    }
   ],
   "source": [
    "print(f'SSE is {SSE(result,q1_train_y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b6cac",
   "metadata": {},
   "source": [
    "c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2bf17a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.43833454e+01,  1.79696033e+00,  1.03270812e+01, -1.15968228e-01,\n",
       "        9.45077486e-01,  5.24042440e-03,  3.00891684e-01])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_inv = np.eye(q1_train_x.shape[1])\n",
    "for i in range(q1_train_x.shape[1]):\n",
    "    delta_inv[i][i] = 1/(norm(Q[:,i])**2)\n",
    "w = np.dot(np.dot(np.dot(np.linalg.inv(R),delta_inv),Q.T),q1_train_y)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf07d6",
   "metadata": {},
   "source": [
    "d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "af2565d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c68956ee88>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(q1_train_x[:,1],q1_train_y)\n",
    "plt.scatter(q1_train_x[:,1],result-sum([proj(q1_train_y,Q[:,i])*Q[:,i] for i in [0,2,3,4,5,6]],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b20816",
   "metadata": {},
   "source": [
    "The blue one is the scatter plot, and the orange one is the regression line from the first attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fb872-6bff-4651-b39d-d0c98d7bb5d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Q2. Logistic Regression: (Total 25 points)  For this question use all attributes as independent, except for the last one, which will be used as the caterogical target or class variable for multiclass logistic regression. \n",
    "\n",
    "##### a) (15 points) Show the weight vector for each class after exactly 4 iterations of **batch** gradient descent. That is, you may not use stochastic gradient descent, but rather you should used the batch gradient computed over all points (given on top of page 633) for each class $j$. Initialize the weight matrix $W$ as follows: \n",
    "\n",
    "[[0.37454012, 0.95071431, 0.73199394],\n",
    "   \n",
    "[0.59865848, 0.15601864, 0.15599452],\n",
    "\n",
    "[0.05808361, 0.86617615, 0.60111501],\n",
    "\n",
    "[0.70807258, 0.02058449, 0.96990985],\n",
    "\n",
    "[0.83244264, 0.21233911, 0.18182497],\n",
    "\n",
    "[0.18340451, 0.30424224, 0.52475643],\n",
    "\n",
    "[0.43194502, 0.29122914, 0.61185289],\n",
    "\n",
    "[0.13949386, 0.29214465, 0.36636184]]\n",
    "\n",
    "##### Where each column of $W$ gives the weight for the corresponding class. Use step size $\\eta = 10^{-5}$. You may use scipy.special.softmax.\n",
    "\n",
    "##### b) (5 points) Compute the cross entropy for the data after exactly 4 iterations.\n",
    "\n",
    "##### c) (5 points) What is the log-odds ratio for class 1 versus class 3 for attribute 1 after 4 iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae653be4",
   "metadata": {},
   "source": [
    "a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0e230c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    num_count = set()\n",
    "    for i in data:\n",
    "        num_count.add(i)\n",
    "    data = np.copy(data)\n",
    "    zeros = np.zeros((data.shape[0],len(num_count)))\n",
    "    for i in range(data.shape[0]):\n",
    "        zeros[i,int(data[i])-1] = 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "013f902a-bf5a-4e0f-8e06-0b7712e9ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_train_x = augment_by_one(np.copy(dataset[:,:-1]))\n",
    "q2_train_y = one_hot_encoding(np.copy(dataset[:,-1]))\n",
    "lr_w = np.array([[0.37454012, 0.95071431, 0.73199394],\n",
    "   \n",
    "[0.59865848, 0.15601864, 0.15599452],\n",
    "\n",
    "[0.05808361, 0.86617615, 0.60111501],\n",
    "\n",
    "[0.70807258, 0.02058449, 0.96990985],\n",
    "\n",
    "[0.83244264, 0.21233911, 0.18182497],\n",
    "\n",
    "[0.18340451, 0.30424224, 0.52475643],\n",
    "\n",
    "[0.43194502, 0.29122914, 0.61185289],\n",
    "\n",
    "[0.13949386, 0.29214465, 0.36636184]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f502167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def multi_class_logistic_regression(initial_weight,eps,lr,max_iter,X,y,print_iter):\n",
    "    weights_prev = np.copy(initial_weight.T)\n",
    "    weights_next = np.copy(weights_prev)\n",
    "    l1_norm = np.Inf\n",
    "    t = 0\n",
    "    index = [i for i in range(X.shape[0])]\n",
    "    while l1_norm > eps and t<max_iter: \n",
    "        np.random.shuffle(index)\n",
    "        grad = [np.zeros(X[0].shape)]*(y.shape[1]-1)\n",
    "        for k_index in index:\n",
    "            pi = softmax(np.dot(weights_next,X[k_index]))\n",
    "            for i in range(y.shape[1]-1):\n",
    "                grad[i] += (y[k_index][i] - pi[i])*X[k_index]\n",
    "        for i in range(y.shape[1]-1):\n",
    "            weights_next[i] = weights_next[i]+lr*grad[i]\n",
    "        t += 1\n",
    "        l1_norm = np.linalg.norm((weights_next - weights_prev))\n",
    "        if t%print_iter ==0:\n",
    "            print(f'iteration {t} with weight difference {l1_norm}')\n",
    "        weights_prev = np.copy(weights_next)\n",
    "    return weights_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1611eda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 with weight difference 0.004048976570231006\n",
      "iteration 2 with weight difference 0.0033955493268850996\n",
      "iteration 3 with weight difference 0.002869472654813818\n",
      "iteration 4 with weight difference 0.0024575472019295248\n"
     ]
    }
   ],
   "source": [
    "weight = multi_class_logistic_regression(lr_w,1e-4,1e-5,4,q2_train_x,q2_train_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d8dfafd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37390839, 0.5959265 , 0.05180733, 0.70757262, 0.82976705,\n",
       "        0.18226472, 0.42780824, 0.13670785],\n",
       "       [0.95008258, 0.15328666, 0.85989987, 0.02008453, 0.20966352,\n",
       "        0.30310245, 0.28709236, 0.28935864],\n",
       "       [0.73199394, 0.15599452, 0.60111501, 0.96990985, 0.18182497,\n",
       "        0.52475643, 0.61185289, 0.36636184]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3cdf8",
   "metadata": {},
   "source": [
    "b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c02c028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y,y_pre):\n",
    "  loss=np.sum(y*np.log(1/y_pre)+(1-y)*np.log(1/(1-y_pre)))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "be72ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(weight,X,y):\n",
    "    y_pred = np.zeros(y.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred[i] = softmax(weight@X[i])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0f7d58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_pred = prediction(weight,q2_train_x,q2_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "94f5a10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492.8108301788774"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(q2_train_y,q2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932f8eb",
   "metadata": {},
   "source": [
    "c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "92752437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.70751656, 65.12083277,  1.46916376, 12.43587185, 77.79277227,\n",
       "        5.92081433, 11.9189697 ,  6.36091829])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_odds_1 = np.zeros(8)\n",
    "for i in range(q2_train_x[:,1].shape[0]):\n",
    "    log_odds_1 += weight[0,:]*q2_train_x[:,1][i]\n",
    "log_odds_3 = np.zeros(8)\n",
    "for i in range(q2_train_x[:,3].shape[0]):\n",
    "    log_odds_3 += weight[2,:]*q2_train_x[:,3][i]\n",
    "log_odds_1/log_odds_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694faece-3214-408a-bbd5-3f8ab535d352",
   "metadata": {},
   "source": [
    "#### Q3. SVM: (Total 25 points) CSCI4390 must use linear kernel, and CSCI6390 must use quadratic homogeneous kernel with $c=0$ for this question. Find the $\\mathbf{\\alpha}$ values via the stochastic GD method for SVM training. Use the last attribute as the class variable, but with +1 denoting class 1, and -1 denoting the other two classes. Set the random seed to 42, and initilize $\\mathbf{\\alpha}$ randomly. Use $C=1$, convergence thrshold $\\epsilon=0.001$ and maxiter=1000. Use hinge loss. Set step size as the inverse of the self-kernel value for each point. Answer the following questions:\n",
    "\n",
    "##### a) (10 points) Print the $\\mathbf{\\alpha}$ vector. How many non-zero values does $\\mathbf{\\alpha}$ have. What do those signify?\n",
    "\n",
    "##### b) (5 points) Find the weight vector $\\mathbf{w}$ and the bias $b$.\n",
    "\n",
    "##### c) (5 points) What is the signed distance of the point at index 70 in the data to the SVM hyperplane $h(\\mathbf{x})=0$ (counting from 0). In what region does the point lie: within or outside the margin, and correctly or incorrectly classified.\n",
    "\n",
    "##### d) (5 points) What is the effective margin of the hyperplane if we ignore incorrectly classified points. What would be the equation of the canonical hyperplane in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e589a40",
   "metadata": {},
   "source": [
    "a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "74aeb231-9bf3-4efb-89f2-7c89a61233c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(threshold,label):\n",
    "    new_label = np.copy(label)\n",
    "    for i,val in enumerate(label):\n",
    "        if (val <= threshold):\n",
    "            new_label[i] = 1\n",
    "        else:\n",
    "            new_label[i] = -1\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "171b2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_train_x = np.copy(dataset[:,:-1])\n",
    "q3_train_y = convert_label(1,np.copy(dataset[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8afa9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def generate_kernel(dataset,kernel,spread = None,dataset1 = None):\n",
    "    if dataset1 == None:\n",
    "        dataset1 = np.copy(dataset)\n",
    "    empty_kernel = np.zeros((dataset.shape[0],dataset.shape[0]))\n",
    "    for i in range(dataset.shape[0]):\n",
    "        for j in range(i,dataset.shape[0]):\n",
    "            X = dataset[i,:]\n",
    "            Y = dataset1[j,:]\n",
    "            if kernel == 'linear':\n",
    "                K_ij = compute_linear_kernel(X,Y)\n",
    "            if kernel == 'gaussian':\n",
    "                K_ij = compute_gaussian_kernel(X,Y,spread)\n",
    "            empty_kernel[i][j] = K_ij\n",
    "            empty_kernel[j][i] = K_ij\n",
    "    return  empty_kernel\n",
    "\n",
    "def compute_linear_kernel(X,Y):\n",
    "    return np.dot(X.T,Y)     \n",
    "def _compute_linear_kernel(X,Y):\n",
    "    return np.dot(X,Y.T)     \n",
    "def compute_gaussian_kernel(X,Y,spread):\n",
    "\treturn np.exp(-1 * (np.linalg.norm(X - Y)**2 / (2 * spread)) )\n",
    "def _gaussian_kernel(data_matrix_1, data_matrix_2, spread):\n",
    "    kernel_matrix = np.zeros((len(data_matrix_1), len(data_matrix_2)))\n",
    "    for i in range(len(data_matrix_1)):\n",
    "        for j in range(len(data_matrix_2)):\n",
    "            numer = np.linalg.norm(data_matrix_1[i] - data_matrix_2[j]) ** 2\n",
    "            denom = float(2 * (spread))\n",
    "            kernel_matrix[i][j] = math.exp(-numer/denom)\n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3eb7ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(x, y, kernel, loss, eps, C, max_iter,spread=None):\n",
    "    if loss == 'hinge':\n",
    "        if kernel == 'linear':\n",
    "            K = generate_kernel(x,kernel)\n",
    "        elif kernel == 'gaussian':\n",
    "            K = generate_kernel(x,kernel,spread)\n",
    "    K = K+1\n",
    "    t = 0\n",
    "    eta = np.asarray([1/k for k in np.diag(K)])\n",
    "    alpha = np.random.rand(K.shape[0])\n",
    "    index = [i for i in range(K.shape[0])]\n",
    "    l1_norm = np.Inf\n",
    "    while l1_norm > eps and t<max_iter:\n",
    "        alpha_prev = np.copy(alpha)\n",
    "        np.random.shuffle(index)\n",
    "        for k_index in index:\n",
    "            alpha[k_index] = alpha[k_index] + eta[k_index] *\\\n",
    "                (1 - y[k_index] * (np.sum(np.multiply(np.multiply(alpha, y), K.T[k_index]))))\n",
    "            if alpha[k_index] < 0:\n",
    "                alpha[k_index] = 0\n",
    "            if loss == \"hinge\" and alpha[k_index] > C:\n",
    "                alpha[k_index] = C\n",
    "        l1_norm = np.linalg.norm((alpha - alpha_prev))\n",
    "        t+=1\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ccefdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_alpha = SVM(q3_train_x,q3_train_y,'linear','hinge',0.001,1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e3eee33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(linear_alpha != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f28d83",
   "metadata": {},
   "source": [
    "This 98 non-zero alpha values suggest that the corresponding 98 vectors are support vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b8bf7",
   "metadata": {},
   "source": [
    "b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e6e98f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.sum((linear_alpha * q3_train_y) * q3_train_x.T, axis=1)\n",
    "b = w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "927451a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.42311579,  1.26358486,  0.18237519,  1.44377095,  0.71541943,\n",
       "       -0.51418414, -3.96848081])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('weight is:')\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b990cbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.42311578986842413"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('bias is:')\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034188c",
   "metadata": {},
   "source": [
    "c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "046c50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the point and the hyperplane is 0.48492066674555284\n"
     ]
    }
   ],
   "source": [
    "r = abs(w@q3_train_x[70,:]+b)/np.linalg.norm(w)\n",
    "print(f'The distance between the point and the hyperplane is {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "25c7c38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1912007945036027"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w@q3_train_x[70,:]+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cf40624c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_train_y[70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fa1e2",
   "metadata": {},
   "source": [
    "The point is correctly classified, and lies inside the margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d16ab2",
   "metadata": {},
   "source": [
    "d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c21cf",
   "metadata": {},
   "source": [
    "To find the canonical hyperplane, we find the farest point correctly labeled. Then we can find the scalar s as well as the margin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6f9c820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_prediction(alpha, y, kernel):\n",
    "    x = np.sign(np.sum((alpha * y * kernel),1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c22f23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pred = svm_prediction(linear_alpha,q3_train_y,_compute_linear_kernel(q3_train_x,q3_train_x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a46eff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7488083506816828\n"
     ]
    }
   ],
   "source": [
    "correct = (linear_pred == q3_train_y)\n",
    "longest_dist = 0\n",
    "for i in range(correct.shape[0]):\n",
    "    if correct[i] == True:\n",
    "        r = abs(w@q3_train_x[i,:]+b)/np.linalg.norm(w)\n",
    "        if (r>longest_dist):\n",
    "            longest_dist = r\n",
    "print(longest_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d18d7f",
   "metadata": {},
   "source": [
    "Thus s is the effective margin is 0.7361044"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6824c",
   "metadata": {},
   "source": [
    "To obtain the canonical hyperplane, we divide the w and b by s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c21ac775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The effective hyperplane has weight:\n",
      "[-0.56505218  1.68746097  0.24355389  1.92809141  0.95541059 -0.68666988\n",
      " -5.29972831]\n",
      "The effective hyperplane has bias:\n",
      "-0.5650521785490743\n"
     ]
    }
   ],
   "source": [
    "print(f'The effective hyperplane has weight:')\n",
    "print(w/longest_dist)\n",
    "print(f'The effective hyperplane has bias:')\n",
    "print(b/longest_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30951748-a587-42ff-b8a8-e8a337f55f2b",
   "metadata": {},
   "source": [
    "#### Q4. Neural Networks: (Total 25 points) Implement a one hidden layer MLP for the seeds dataset, where the target is the class variable. That is, you must use softmax activation for the output layer, with cross-entropy (CE) loss.  Use stochastic GD. Note that for the output layer, the net gradient $\\mathbf{\\delta}_o = \\mathbf{o} - \\mathbf{y}$ for softmax and CE loss (see eq. 25.52). However, for the hidden layer, the net gradient $\\delta_h$ is the derivative of the ReLU function multiplied elementwise with $(\\mathbf{W}_o \\mathbf{\\delta}_o)$.  Answer the following questions:\n",
    "\n",
    "##### a) (15 points) Implement the MLP model with one hidden layer. Set the random seed to 42, and initialize the weight matrices and bias vectors randomly. For the hidden layer use $m=16$ neurons, with ReLU activation. Use $\\eta=0.001$. Train for 200 iterations. Print the parameters of the model: $\\mathbf{b}_h, \\mathbf{W}_h, \\mathbf{b}_o, \\mathbf{W}_o$ after training.\n",
    "\n",
    "##### b) (5 points) What is the cross-entropy and accuracy of your model.\n",
    "\n",
    "##### c) (5 points) i) For the first point in the data (at index 0), what is the hidden layer value, and output vector (after model has been trained)? ii) What is the CE loss for the first point? iii) Print the net gradient vectors at the output and hidden layer for the first point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "31392b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_train_x = np.copy(dataset[:,:-1])\n",
    "q4_train_y = one_hot_encoding(np.copy(dataset[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "db4b1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu_forward(input):\n",
    "    relu_forward = np.maximum(0,input)\n",
    "    return relu_forward\n",
    "def dense_forward(weight,bias,inputs):\n",
    "    np.dot(inputs,weight) + bias\n",
    "def Relu_backward(input):\n",
    "    relu_grad = input > 0\n",
    "    return relu_grad\n",
    "eta = 0.001\n",
    "n_0 = 7\n",
    "n_3 = 3\n",
    "lis_b = []\n",
    "lis_h = []\n",
    "lis_b.append(np.random.rand(16))\n",
    "lis_b.append(np.random.rand(3))\n",
    "lis_h.append(np.random.rand(7,16))\n",
    "lis_h.append(np.random.rand(16,3))\n",
    "t = 0\n",
    "index = [i for i in range(q4_train_x.shape[0])]\n",
    "while t<=200:\n",
    "    np.random.shuffle(index)\n",
    "    for k in index:\n",
    "        z_l = q4_train_x[k,:]\n",
    "        zl_list = []\n",
    "        for i in range(2):\n",
    "            zl_list.append(z_l)\n",
    "            z_l = Relu_forward(np.dot(z_l,lis_h[i]) + lis_b[i])\n",
    "        output = softmax(z_l)\n",
    "        grad_2 = output - q4_train_y[k,:]\n",
    "        # for i in [1,0]:\n",
    "        #     if i == 1:\n",
    "        grad_1 = np.multiply(Relu_backward(zl_list[1]),np.dot(lis_h[1],grad_2))\n",
    "            # elif i ==0:\n",
    "            #     grad_0 = np.multiply(Relu_backward(zl_list[i]),np.dot(lis_h[i],grad_1))\n",
    "        for i in range(2):\n",
    "            if (i ==0):\n",
    "                lis_h[i] = lis_h[i]-eta*np.dot(zl_list[0].reshape(-1,1),grad_1.T.reshape(1,-1))\n",
    "                lis_b[i] = lis_b[i]-eta*grad_1\n",
    "            elif (i == 1):\n",
    "                lis_h[i] = lis_h[i]-eta*np.dot(zl_list[1].reshape(-1,1),grad_2.T.reshape(1,-1))\n",
    "                lis_b[i] = lis_b[i]-eta*grad_2\n",
    "    t += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "df98f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "for k in index:\n",
    "    z_l = q4_train_x[k,:]\n",
    "    zl_list = []\n",
    "    for i in range(2):\n",
    "        zl_list.append(z_l)\n",
    "        z_l = Relu_forward(np.dot(z_l,lis_h[i]) + lis_b[i])\n",
    "    output = softmax(z_l)\n",
    "    results.append(output)\n",
    "q4_pred = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7695ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy is 1452.9083306066395\n"
     ]
    }
   ],
   "source": [
    "print(f'cross entropy is {cross_entropy(q4_train_y,q4_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d99fbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_result = list()\n",
    "for i in results:\n",
    "    arg_result.append(np.argmax(i)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3e75b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.38095238095238093\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy is {np.sum(dataset[:,-1] == arg_result)/dataset.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad7544",
   "metadata": {},
   "source": [
    "c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "137f1279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hidden layer value is\n",
      "[5.25795992 4.2316791  4.70049207 0.4028343  0.66194869 4.19578466\n",
      " 4.03386129 2.021178   2.65311084 0.         1.27560401 0.09495677\n",
      " 2.66426521 0.         0.87633133 2.50189946]\n",
      "the output vector is\n",
      "[9.69574381e-01 2.95484873e-02 8.77131952e-04]\n",
      "the cross entropy is\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Micha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "z_l = q4_train_x[0,:]\n",
    "hidden_layer_value = Relu_forward(np.dot(z_l,lis_h[0]) + lis_b[0])\n",
    "for i in range(2):\n",
    "    z_l = Relu_forward(np.dot(z_l,lis_h[i]) + lis_b[i])\n",
    "output = softmax(z_l)\n",
    "ce = cross_entropy(output,q4_train_y[0,:])\n",
    "print('The hidden layer value is')\n",
    "print(hidden_layer_value)\n",
    "print('the output vector is')\n",
    "print(output)\n",
    "print('the cross entropy is')\n",
    "print(ce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1583098d49b8b63b24ba78fec75676b625853923384d7c0eb5f87db36da8a08e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
